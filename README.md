# tec-CHAT

NLP app done with the Tec1, MINT and Speech chip.   

In 1966, program ELIZA was developed, which aimed at tricking it users by making them believe that they were having a conversation with a real human being. ELIZA was designed to imitate a therapist who would ask open-ended questions and even respond with follow-ups. It responds to questions with answers that sound like they were provided by a therapist. The program is designed to mimic human conversation, and it is often used to gull people into thinking they are talking to a real person. HAT DOES ELIZA DO? Using "'pattern matching" and substitution methodology, the program gives canned responses that made early users feel they were talking to someone who understood their input. The program was limited by the scripts that were in the program. (ELIZA was originally written in MAD-Slip.) 

 

## NLP; GPT-3, T5, Transformers

GPT-3 is a computer system that is designed to generate human-like responses to questions. It is based on a neural network that has been trained on a large amount of data. The system is designed to generate responses that are similar to what a human would say. GPT-3 is the third generation GPT Natural Language Processing model created by OpenAI. It is the size that differentiates GPT-3 from its predecessors. The 175 billion parameters of GPT-3 make it 17 times as large as GPT-2. It also turns GPT-3 about ten times as large as Microsoft's Turing NLG model. [6]

This website [3] provides a simple way to generate text using the T5 transformer. You simply provide a prompt, and the transformer will generate text based on the prompt. The text generation is based on the T5 transformer, which is a neural network model that is trained on a large amount of text data. The transformer learn the relationships between the words in the text data, and can then generate new text that is similar to the training data. The T5 transformer is a neural network model that is trained on a large amount of text data. The transformer learn the relationships between the words in the text data, and can then generate new text that is similar to the training data.and can generate text on its own.it can also improve the quality of the text it generates by fine-tuning the model on specific domains or tasks and can generate text in multiple languages and multiple genres, that is both grammatically correct and fluent. 

Google is using the T5 text-to-text transfer learning algorithm to improve the performance of its natural language processing models. The T5 algorithm can be used to fine-tune a model to a specific task, and this can improve the performance of the model on that task. For example, the T5 algorithm was used to improve the performance of a Google Translate model. The T5 algorithm can also be used to improve the performance of other NLP tasks, such as question answering and text classification. The T5 algorithm is a powerful tool for transfer learning, and it can be used to improve the performance of many different types of NLP models. [4]

Text-to-text transfer transformer (T5) is a new approach for natural language understanding (NLU) that can be used to read and comprehend any text. It was developed by Google Research and is based on the transformer architecture. T5 is trained on a large amount of text data in order to learn the general knowledge about the world. This allows it to transfer that knowledge to any text, regardless of the domain or the task. For example, T5 can be used to generate summaries of news articles, generate descriptions of images, or answer questions based on a passage of text. T5 has shown promising results on a variety of NLU tasks, including question answering, text classification, and text generation. In addition, T5 is efficient to train and can be used on a variety of hardware platforms, including CPUs, GPUs, and TPUs. Overall, T5 is a promising new approach for NLU that has the potential to revolutionize the field.[5]

## Speech on the tec-1

We have https://github.com/SteveJustin1963/tec-SPEECH that allows us to make phonetic sounds, but we have to write the sound strings to drive the chip. We need a very simple ai system. We could write code to call an API from a paid site like OpenAI.com to do the hard lifting, but thats not fun for the Z80. 


## Ref 
1. https://en.wikipedia.org/wiki/ELIZA
2. https://web.njit.edu/~ronkowit/eliza.html
3. https://towardsdatascience.com/poor-mans-gpt-3-few-shot-text-generation-with-t5-transformer-51f1b01f843e
4. https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html
5. https://github.com/google-research/text-to-text-transfer-transformer
6. https://www.itbusinessedge.com/development/what-is-gpt-3/
